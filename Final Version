# Import necessary libraries
import whisper
import torch
import streamlit as st
from langdetect import detect
from transformers import MarianMTModel, MarianTokenizer
import re
import librosa
import scipy.fft
import cv2
import os
import shutil
from transformers import pipeline
from moviepy.editor import VideoFileClip
from torchvision import models, transforms
import matplotlib.pyplot as plt
import easyocr

# Page configuration
st.set_page_config(page_title="Unified Media Analysis Platform", layout="wide")

# Load models with caching (to avoid reloading on every run)
@st.cache_resource
def load_models():
    whisper_model = whisper.load_model("base")
    text_classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
    vision_model = models.resnet50(pretrained=True)
    vision_model.eval()
    return whisper_model, text_classifier, vision_model

# Initialize models
whisper_model, text_classifier, vision_model = load_models()

# Helper function: check if text is predominantly Chinese (to improve language detection)
def is_probably_chinese(text):
    chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
    return len(chinese_chars) / max(len(text), 1) > 0.3

# Helper function: translate text to English (returns translated text and detected language code)
def translate_to_english(text):
    try:
        # Detect language (special handling for Chinese)
        if is_probably_chinese(text):
            lang = 'zh'
        else:
            lang = detect(text)
        if lang.startswith('en'):
            # Already in English
            return text, lang
        # Load translation model for detected language to English
        model_name = f"Helsinki-NLP/opus-mt-{lang}-en"
        tokenizer = MarianTokenizer.from_pretrained(model_name)
        model = MarianMTModel.from_pretrained(model_name)
        translated = model.generate(**tokenizer(text, return_tensors='pt', padding=True))
        english_text = tokenizer.decode(translated[0], skip_special_tokens=True)
        return english_text, lang
    except Exception:
        return text, "und"

# Helper function: get OCR reader for a given language code (ensures English is always included)
def get_ocr_reader_by_lang(lang_code):
    lang_map = {
        "en": ["en"],
        "es": ["en", "es"],
        "fr": ["en", "fr"],
        "ar": ["en", "ar"],
        "ru": ["en", "ru"],
        "zh": ["en", "ch_sim"]
    }
    langs = lang_map.get(lang_code, ["en"])
    return easyocr.Reader(langs, gpu=False)

# Helper function: transcribe audio using Whisper model
def transcribe_audio(file_path, model):
    result = model.transcribe(file_path)
    return result['text']

# Helper function: analyze text with zero-shot classification (returns labels and scores)
def analyze_text(text, classifier):
    labels = ["misinformation", "biased language", "xenophobic statement", "neutral"]
    return classifier(text, labels)

# Helper function: specifically detect xenophobia (subset of zero-shot classification)
def detect_xenophobia(text, classifier):
    labels = ["neutral", "xenophobic", "biased"]
    return classifier(text, labels)

# Helper function: analyze basic audio features (FFT, stats, placeholder emotion)
def audio_analysis(file_path):
    audio, sr = librosa.load(file_path)
    fft = scipy.fft.fft(audio)
    fft_freq = scipy.fft.fftfreq(len(audio), 1 / sr)
    return {
        "fft": fft,
        "fft_freq": fft_freq,
        "mean": float(audio.mean()),
        "max": float(audio.max()),
        "min": float(audio.min()),
        "std": float(audio.std()),
        "emotion": "neutral"
    }

# Helper function: extract video frames at a given interval (in seconds)
def extract_frames(video_path, output_folder="frames", interval=1):
    if os.path.exists(output_folder):
        shutil.rmtree(output_folder)
    os.makedirs(output_folder, exist_ok=True)
    vidcap = cv2.VideoCapture(video_path)
    fps = vidcap.get(cv2.CAP_PROP_FPS)
    success, image = vidcap.read()
    count, frame_count = 0, 0
    while success:
        # Save frame at specified time interval
        if count % int(fps * interval) == 0:
            frame_path = os.path.join(output_folder, f"frame{frame_count}.jpg")
            cv2.imwrite(frame_path, image)
            frame_count += 1
        success, image = vidcap.read()
        count += 1
    vidcap.release()

# Helper function: analyze extracted frames using vision model (returns list of (frame, class, probability))
def analyze_frames(folder, model):
    preprocess = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor()
    ])
    frame_data = []
    for filename in sorted(os.listdir(folder)):
        if filename.endswith(".jpg"):
            image = cv2.imread(os.path.join(folder, filename))
            input_tensor = preprocess(image).unsqueeze(0)
            with torch.no_grad():
                output = model(input_tensor)
            preds = torch.nn.functional.softmax(output[0], dim=0)
            top_prob, top_class = torch.max(preds, 0)
            frame_data.append((filename, int(top_class.item()), float(top_prob.item())))
    return frame_data

# Helper function: clean OCR text by removing non-alphanumeric characters (keeps English and Chinese characters)
def clean_ocr_text(raw_text):
    return re.sub(r"[^a-zA-Z0-9‰∏Ä-Èæ•\s.,:;!?%\-]", "", raw_text)

# Helper function: determine if a frame is usable for OCR (exclude very dark/blank frames)
def is_frame_usable(image_path, threshold=30):
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        return False
    mean_brightness = img.mean()
    return mean_brightness > threshold

# Helper function: perform OCR on all usable frames and return concatenated text
def perform_ocr_on_frames(folder, reader):
    seen = set()
    texts = []
    for filename in sorted(os.listdir(folder)):
        if filename.endswith(".jpg"):
            image_path = os.path.join(folder, filename)
            if is_frame_usable(image_path):
                results = reader.readtext(image_path, detail=0)
                for line in results:
                    cleaned = clean_ocr_text(line.strip())
                    if cleaned and cleaned not in seen and len(cleaned) > 20:
                        seen.add(cleaned)
                        texts.append(cleaned)
    return " ".join(texts)

# Page header (title and subtitle with HTML for styling)
st.markdown("""
<h1 style='text-align: center; color: #2C3E50; font-size: 42px; font-weight: bold;'>
Unified Media Analysis Platform
</h1>
<h3 style='text-align: center; color: #7F8C8D; font-size: 22px; font-weight: normal;'>
Multilingual Audio, Video, and Text Analysis with Dynamic OCR
</h3>
<hr style='border-top: 1px solid #bbb;'>
""", unsafe_allow_html=True)

# Sidebar - file upload and settings
st.sidebar.header("Upload File")
uploaded_file = st.sidebar.file_uploader("Upload an Audio/Video file", type=["mp4", "wav", "mp3"])
st.sidebar.markdown("---")
st.sidebar.header("Settings")
frame_interval = st.sidebar.slider("Frame Extraction Interval (seconds)", 1, 10, 1)

# Main processing logic (runs after a file is uploaded)
if uploaded_file:
    with st.spinner('Analyzing...'):
        # Save the uploaded file to a temporary location
        file_path = "temp_uploaded_file"
        with open(file_path, "wb") as f:
            f.write(uploaded_file.read())
        # If the file is a video, extract audio track
        audio_path = file_path
        if uploaded_file.name.endswith(".mp4"):
            video = VideoFileClip(file_path)
            audio_path = "extracted_audio.wav"
            video.audio.write_audiofile(audio_path, verbose=False, logger=None)
        # Perform audio transcription and language translation
        transcript = transcribe_audio(audio_path, whisper_model)
        translated_text, detected_language = translate_to_english(transcript)
        # Run text analysis and xenophobia detection on translated text
        text_analysis_result = analyze_text(translated_text, text_classifier)
        xenophobia_result = detect_xenophobia(translated_text, text_classifier)
        # Initialize variables for video analysis
        ocr_text = ""
        ocr_analysis_result = {}
        frame_analysis_result = []
        # If a video file, process frames for classification and OCR
        if uploaded_file.name.endswith(".mp4"):
            extract_frames(file_path, interval=frame_interval)
            frame_analysis_result = analyze_frames("frames", vision_model)
            ocr_reader = get_ocr_reader_by_lang(detected_language)
            ocr_text = perform_ocr_on_frames("frames", ocr_reader)
            translated_ocr, _ = translate_to_english(ocr_text)
            ocr_analysis_result = analyze_text(translated_ocr, text_classifier)

    # Create tabs to display results
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["üéµ Audio", "üìù Transcript", "üåé Xenophobia", "üé• Video", "üîç OCR"])

    # Tab 1: Audio features
    with tab1:
        st.header("Audio Features")
        # Compute audio analysis (FFT and statistics)
        audio_result = audio_analysis(audio_path)
        st.metric("Mean", round(audio_result["mean"], 4))
        st.metric("Max", round(audio_result["max"], 4))
        st.metric("Min", round(audio_result["min"], 4))
        st.metric("Std Dev", round(audio_result["std"], 4))
        st.metric("Emotion Prediction", audio_result["emotion"])
        # Plot audio frequency spectrum (first 500 frequencies for clarity)
        fig, ax = plt.subplots()
        ax.plot(audio_result["fft_freq"][:500], abs(audio_result["fft"][:500]))
        ax.set_xlabel("Frequency (Hz)")
        ax.set_ylabel("Amplitude")
        st.pyplot(fig)

    # Tab 2: Transcription and NLP analysis
    with tab2:
        st.header("Transcription and NLP")
        st.info(f"Detected Language: {detected_language}")
        st.text_area("Original Transcript", transcript, height=200)
        st.text_area("Translated to English", translated_text, height=200)
        st.subheader("Text Analysis")
        st.json(text_analysis_result)

    # Tab 3: Xenophobia detection results
    with tab3:
        st.header("Xenophobia Detection")
        st.json(xenophobia_result)

    # Tab 4: Video frame analysis (if applicable)
    with tab4:
        st.header("Video Frame Analysis")
        if uploaded_file.name.endswith(".mp4"):
            for frame_name, cls, prob in frame_analysis_result[:10]:
                st.image(f"frames/{frame_name}", caption=f"Class: {cls}, Confidence: {prob:.2f}", width=300)

    # Tab 5: OCR results and analysis
    with tab5:
        st.header("OCR Result")
        if ocr_text:
            st.text_area("Extracted Text", ocr_text, height=200)
            st.json(ocr_analysis_result)
        else:
            st.info("No OCR results available.")
